Had to install Ollama to run models locally.

ollama pull qwen2:7b

start ollama in the background:
ollama serve

use liteLLMModel instead of HfApiModel

from smolagents import liteLLMModel
model = LiteLLMModel(
  model_id = "ollama_chat/qwen2.5:7b",
  api_base = "http://127.0.0.1:11434",
  num_ctx=8192,
)

Why the LiteLLMModel works - 
Ollama serves models locally using OpenAI-compatible API at http://localhost:11434
LiteLLMModel is built to communicate with any model that supports the Open AI chat completion API format.
This means you can simply swap out HfApiModel for LiteLLMModel no other code changes required. It's a seamless plug and play solution.
